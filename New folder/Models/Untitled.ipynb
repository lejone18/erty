{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ca43ae9f-8267-45ee-97be-cf6bae8b6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a5c440ca-d302-40fa-b660-49afcf96e4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bullying_words</th>\n",
       "      <th>type_bully</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>bully</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      bullying_words type_bully\n",
       "0                             Get fucking real dude.      bully\n",
       "1   She is as dirty as they come  and that crook ...      bully\n",
       "2   why did you fuck it up. I could do it all day...      bully\n",
       "3   Dude they dont finish enclosing the fucking s...      bully\n",
       "4   WTF are you talking about Men? No men thats n...      bully"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"new.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e438bc91-9ba7-495a-8614-0cdfeb9a94de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fucking real dude'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processing data from commentsit being bullying_words\n",
    "import spacy  \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# function to preprocess\n",
    "def preprocess(bullying_words):\n",
    "    \n",
    "    doc = nlp(bullying_words)\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        # appending new values to the list\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    # converting alist to a string\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "preprocess(\"Get fucking real dude.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4fb0afa2-d1cc-4eb0-a320-bcaceb1fa03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_bully\n",
       "non-bully    12245\n",
       "bully         7980\n",
       "non             15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if they balance\n",
    "df.type_bully.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6fe97506-7b44-477f-b65d-0cdfc900411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# under sampling taking random sample\n",
    "min_sample = 7980\n",
    "df_bully = df[df.type_bully == \"bully\"].sample(min_sample, random_state=40)\n",
    "df_non_bully = df[df.type_bully == \"non-bully\"].sample(min_sample, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7ecc422a-e0b9-4392-95cf-58ad1b2ce3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_bully\n",
       "bully        7980\n",
       "non-bully    7980\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it supply bunch of data frames into argument and add them row by row\n",
    "df_balanced = pd.concat([df_bully, df_non_bully], axis=0)\n",
    "df_balanced.type_bully.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ab2607b0-2152-498b-9fcc-793ae3088d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bullying_words</th>\n",
       "      <th>type_bully</th>\n",
       "      <th>type_bully_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4656</th>\n",
       "      <td>whats the operating system on it? i have windo...</td>\n",
       "      <td>bully</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>Lol. Ur a bitch. I've done Hondurans and Chile...</td>\n",
       "      <td>bully</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>You didn't imagine it. It was the \"get away fr...</td>\n",
       "      <td>bully</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7734</th>\n",
       "      <td>shits gay!</td>\n",
       "      <td>bully</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>people hate to admit emotional decision making...</td>\n",
       "      <td>bully</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         bullying_words type_bully  \\\n",
       "4656  whats the operating system on it? i have windo...      bully   \n",
       "6129  Lol. Ur a bitch. I've done Hondurans and Chile...      bully   \n",
       "3506  You didn't imagine it. It was the \"get away fr...      bully   \n",
       "7734                                         shits gay!      bully   \n",
       "4450  people hate to admit emotional decision making...      bully   \n",
       "\n",
       "      type_bully_num  \n",
       "4656               1  \n",
       "6129               1  \n",
       "3506               1  \n",
       "7734               1  \n",
       "4450               1  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting into numeric when it maps bully and non-bully\n",
    "df_balanced['type_bully_num'] = df_balanced.type_bully.map(\n",
    "    {'bully': 1, 'non-bully': 0\n",
    "})\n",
    "# checking it\n",
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "eef54fea-7b97-43ac-b48c-ed17f9f37d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Assuming df is your DataFrame containing 'bullying_words' and 'type_bully' columns\n",
    "# # Fill missing values with an empty string\n",
    "# df['bullying_words'] = df['bullying_words'].fillna('')\n",
    "\n",
    "# # Tokenization\n",
    "# df['tokenized_text'] = df['bullying_words'].apply(word_tokenize)\n",
    "\n",
    "# # Remove stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['filtered_text'] = df['tokenized_text'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
    "\n",
    "# # Convert text to TF-IDF vectors\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# X = tfidf_vectorizer.fit_transform(df['filtered_text'].apply(lambda tokens: ' '.join(tokens)))\n",
    "# Y = df['type_bully']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a21f77d1-c3da-424c-8838-7383033efb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[:-1]].values\n",
    "Y = df[df.columns[1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "41208498-7854-4a19-bc2e-3a048acf354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6f5671d0-20dd-4754-b499-9746186e30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a model and the column type_bully_num is our target varviable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming df is your DataFrame containing 'bullying_words' and 'type_bully' columns\n",
    "# Fill missing values with an empty string\n",
    "df['bullying_words'] = df['bullying_words'].fillna('')\n",
    "\n",
    "# Tokenization\n",
    "df['tokenized_text'] = df['bullying_words'].apply(word_tokenize)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['filtered_text'] = df['tokenized_text'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
    "\n",
    "# Convert text to TF-IDF vectors\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(df['filtered_text'].apply(lambda tokens: ' '.join(tokens)))\n",
    "Y = df['type_bully']\n",
    "\n",
    "# Split the data into training and testing sets(60% of data for training, 20% for validation, 20% for testing\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "# it take unit of the data set and take it into 50/50\n",
    "X_valid, X_test, Y_valid, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f33a44fd-5519-4ffb-a574-77ef03e8aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    # layer it takes everything and output a value it has 16 layer\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "730f4a2e-c553-4247-ac80-07aa7a784a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "             loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d7adf352-9caf-4209-9f0d-549b25264090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.3763 - loss: 0.6976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6975890398025513, 0.3754940629005432]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Encode string labels to numerical values\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "Y_test_encoded = label_encoder.transform(Y_test)\n",
    "\n",
    "# Now you can use Y_train_encoded and Y_test_encoded for training and evaluation\n",
    "model.evaluate(X_train, Y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1ffa1eb7-0093-467a-95e9-0fd14764182a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3793 - loss: 0.6976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6974648237228394, 0.38191699981689453]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Assuming Y_valid contains string labels\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Encode string labels to numerical values\n",
    "Y_valid_encoded = label_encoder.fit_transform(Y_valid)\n",
    "# Now you can use X_valid and Y_valid_encoded for evaluation\n",
    "model.evaluate(X_valid, Y_valid_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b3e36e50-026e-4093-b17c-df467ca349c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.0086 - loss: -0.9640 - val_accuracy: 2.4704e-04 - val_loss: -19.2876\n",
      "Epoch 2/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 6.1081e-04 - loss: -45.0983 - val_accuracy: 0.0020 - val_loss: -153.1577\n",
      "Epoch 3/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.0018 - loss: -256.8987 - val_accuracy: 0.0022 - val_loss: -487.0391\n",
      "Epoch 4/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0036 - loss: -729.2184 - val_accuracy: 0.0025 - val_loss: -1070.7749\n",
      "Epoch 5/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0040 - loss: -1504.1384 - val_accuracy: 0.0025 - val_loss: -1917.8827\n",
      "Epoch 6/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0035 - loss: -2662.7646 - val_accuracy: 0.0027 - val_loss: -3051.5583\n",
      "Epoch 7/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.0049 - loss: -3934.1096 - val_accuracy: 0.0030 - val_loss: -4514.7900\n",
      "Epoch 8/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0038 - loss: -5843.0400 - val_accuracy: 0.0030 - val_loss: -6325.6968\n",
      "Epoch 9/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0050 - loss: -8228.4639 - val_accuracy: 0.0030 - val_loss: -8492.0908\n",
      "Epoch 10/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0042 - loss: -11153.4238 - val_accuracy: 0.0037 - val_loss: -11061.7822\n",
      "Epoch 11/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.0045 - loss: -14423.2930 - val_accuracy: 0.0040 - val_loss: -14043.0664\n",
      "Epoch 12/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0053 - loss: -18465.8633 - val_accuracy: 0.0037 - val_loss: -17492.6797\n",
      "Epoch 13/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.0059 - loss: -21963.5820 - val_accuracy: 0.0037 - val_loss: -21430.2891\n",
      "Epoch 14/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0047 - loss: -28338.8301 - val_accuracy: 0.0040 - val_loss: -25805.9883\n",
      "Epoch 15/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0053 - loss: -34677.6406 - val_accuracy: 0.0040 - val_loss: -30685.6816\n",
      "Epoch 16/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0049 - loss: -39218.1445 - val_accuracy: 0.0040 - val_loss: -36144.3281\n",
      "Epoch 17/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0052 - loss: -44228.6719 - val_accuracy: 0.0040 - val_loss: -42227.8242\n",
      "Epoch 18/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0047 - loss: -55587.6797 - val_accuracy: 0.0037 - val_loss: -48916.6836\n",
      "Epoch 19/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.0036 - loss: -62984.3320 - val_accuracy: 0.0042 - val_loss: -56173.4609\n",
      "Epoch 20/20\n",
      "\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.0047 - loss: -71399.5625 - val_accuracy: 0.0037 - val_loss: -64110.8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21cd9dc85f0>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X_train, Y_train, batch_size=16, epochs=20, validation_data=(X_valid, Y_valid)) \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode string labels to numerical values for both training and validation sets\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "Y_valid_encoded = label_encoder.transform(Y_valid)\n",
    "\n",
    "# Now you can use X_train, Y_train_encoded, X_valid, and Y_valid_encoded for model training and evaluation\n",
    "model.fit(X_train, Y_train_encoded, batch_size=16, epochs=20, validation_data=(X_valid, Y_valid_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "477c6ddb-6944-4bba-b46b-2db64ab9c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02      1549\n",
      "           1       0.00      1.00      0.00         2\n",
      "           2       1.00      0.00      0.00      2497\n",
      "\n",
      "    accuracy                           0.00      4048\n",
      "   macro avg       0.67      0.34      0.01      4048\n",
      "weighted avg       1.00      0.00      0.01      4048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming Y_test and Y_pred are your true and predicted labels\n",
    "# Convert Y_test to numeric labels if they are strings\n",
    "if Y_test.dtype == 'object':\n",
    "    label_mapping = {label: idx for idx, label in enumerate(np.unique(Y_test))}\n",
    "    Y_test = np.array([label_mapping[label] for label in Y_test])\n",
    "\n",
    "# Convert Y_pred to numeric labels if they are probabilities\n",
    "if Y_pred.shape[1] > 1:  # If Y_pred is probabilities, convert to class predictions\n",
    "    Y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Generate the classification report with zero_division=1\n",
    "report = classification_report(Y_test, Y_pred, zero_division=1)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1075ba70-8be4-4346-b550-56354f35d6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003952569169960474"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b179d4d2-a4c1-4599-b6c9-b6e462aa39c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_bully\n",
       "non-bully    7324\n",
       "bully        4808\n",
       "non            12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65621c55-bdc8-4297-95aa-4e1b864dffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
